{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "XGBoost and LightGBM v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJeAV1Z4JBIn"
      },
      "source": [
        "## Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHT0A3TTJFXf"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import linalg, optimize\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "import string\n",
        "from nltk.corpus import wordnet as wn\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk import word_tokenize\n",
        "from scipy import optimize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from scipy.sparse import coo_matrix, hstack, vstack\n",
        "import seaborn as sns\n",
        "from textblob import TextBlob\n",
        "# Graphing: \n",
        "%matplotlib inline\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-oUbBciJbfi",
        "outputId": "9b76f3a6-2fd6-4b5a-829f-68ba543f31e3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls \"/content/drive/My Drive/BT4222 Data/training_set.csv\"\n",
        "path = '/content/drive/My Drive/BT4222 Data/training_set.csv'\n",
        "training_set = pd.read_csv(path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "'/content/drive/My Drive/BT4222 Data/training_set.csv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nXnuCMPLKqO",
        "outputId": "185379d2-ee65-410e-a214-00e555ea23b0"
      },
      "source": [
        "!ls \"/content/drive/My Drive/BT4222 Data/training_labels.csv\"\n",
        "path = '/content/drive/My Drive/BT4222 Data/training_labels.csv'\n",
        "training_labels = pd.read_csv(path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/drive/My Drive/BT4222 Data/training_labels.csv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT-QR6Y2LRSP",
        "outputId": "2728785c-1e11-4532-9cbe-69c48ef59fa0"
      },
      "source": [
        "!ls \"/content/drive/My Drive/BT4222 Data/testing_set.csv\"\n",
        "path = '/content/drive/My Drive/BT4222 Data/testing_set.csv'\n",
        "testing_set = pd.read_csv(path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/drive/My Drive/BT4222 Data/testing_set.csv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp8NNoYiLZ-Y",
        "outputId": "cc163ffe-80b0-422d-ecb8-ef210c81ae87"
      },
      "source": [
        "!ls \"/content/drive/My Drive/BT4222 Data/testing_labels.csv\"\n",
        "path = '/content/drive/My Drive/BT4222 Data/testing_labels.csv'\n",
        "testing_labels = pd.read_csv(path)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/drive/My Drive/BT4222 Data/testing_labels.csv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upgWfrPVe6Rg",
        "outputId": "68203fe3-bedb-4109-d116-678a30d6d58c"
      },
      "source": [
        "!ls \"/content/drive/My Drive/BT4222 Data/combined.csv\"\n",
        "path = '/content/drive/My Drive/BT4222 Data/combined.csv'\n",
        "combined = pd.read_csv(path)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/drive/My Drive/BT4222 Data/combined.csv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZPhoGzUNXXT"
      },
      "source": [
        "## LightGBM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SUv4Ph89C1I"
      },
      "source": [
        "> With GridSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPH6WOy495go"
      },
      "source": [
        "* Training Errors\n",
        "> Mean Squared Error:  0.05236697071909393\n",
        "> RMSE:  0.2288383069311035\n",
        "> Mean Absolute Error:  0.05840637214862351\n",
        "> R-squared:  0.9476325904944419\n",
        "> Adjusted R2:  0.9476264467117445\n",
        "* Test Errors\n",
        "> Mean Squared Error:  0.05968673906035098\n",
        "> RMSE:  0.24430869624381155\n",
        "> Mean Absolute Error:  0.06415639168013557\n",
        "> R-squared:  0.9335835121117316\n",
        "> Adjusted R2:  0.9335133150084223"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vzojqoj94Qs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH09BqkINThZ",
        "outputId": "360f374e-6ee6-4ab7-bdee-37a059582a6b"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import preprocessing\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "# Initialise LGBMRegressor\n",
        "# rf = LGBMRegressor(n_estimators=300, subsample=0.9, colsample_bytree=0.2, eval_metric = 'rmse')\n",
        "rf = LGBMRegressor(n_estimators=300, eval_metric = 'rmse')\n",
        "\n",
        "\n",
        "# Set up 5-fold cross-validation\n",
        "from sklearn import model_selection\n",
        "cv = model_selection.KFold(5)\n",
        "\n",
        "\n",
        "# Pipeline Standardization and Model\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n",
        "                           , ('model', rf) ])\n",
        "# Tuning the model\n",
        "my_min_samples_leaf = [2, 10, 25, 50, 100]\n",
        "my_max_depth = [7, 8, 9, 10, 11, 12]\n",
        "my_subsample = [0.7, 0.8, 0.9]\n",
        "my_colsample_bytree = [0.1, 0.2, 0.3]\n",
        "\n",
        "# Run the model using GridSearch, select the model with best search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "optimized_rf = GridSearchCV(estimator=pipeline\n",
        "                            , cv=cv\n",
        "                            , param_grid =dict(model__min_samples_leaf = my_min_samples_leaf, \n",
        "                                               model__max_depth = my_max_depth,\n",
        "                                               model__subsample = my_subsample,\n",
        "                                               model__colsample_bytree = my_colsample_bytree)\n",
        "                            , scoring = 'neg_mean_squared_error'\n",
        "                            , verbose = 1\n",
        "                            , n_jobs = -1\n",
        "                           )\n",
        "\n",
        "# Fitting on Training Data\n",
        "optimized_rf.fit(training_set, training_labels)\n",
        "\n",
        "\n",
        "# Best model estimators\n",
        "print(optimized_rf.best_estimator_)\n",
        "\n",
        "def adjusted_r2(r2, n, p):\n",
        "  result = 1-((1-r2)*((n-1)/(n-p-1)))\n",
        "  return result\n",
        "\n",
        "\n",
        "# Evaluate metrics on holdout\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "y_train_pred = optimized_rf.predict(training_set)\n",
        "y_pred = optimized_rf.predict(testing_set)\n",
        "n1 = len(y_train_pred)\n",
        "p1 = training_set.shape[1]\n",
        "n2 = len(y_pred)\n",
        "p2 = testing_set.shape[1]\n",
        "print('Training Errors')\n",
        "print(\"Mean Squared Error: \", mean_squared_error(training_labels, y_train_pred))\n",
        "print(\"RMSE: \", mean_squared_error(training_labels, y_train_pred, squared = False))\n",
        "print(\"Mean Absolute Error: \", mean_absolute_error(training_labels, y_train_pred))\n",
        "print(\"R-squared: \", r2_score(training_labels, y_train_pred))\n",
        "print(\"Adjusted R2: \", adjusted_r2(r2_score(training_labels, y_train_pred), n1, p1))\n",
        "\n",
        "print('Test Errors')\n",
        "print(\"Mean Squared Error: \", mean_squared_error(testing_labels, y_pred))\n",
        "print(\"RMSE: \", mean_squared_error(testing_labels, y_pred, squared = False))\n",
        "print(\"Mean Absolute Error: \", mean_absolute_error(testing_labels, y_pred))\n",
        "print(\"R-squared: \", r2_score(testing_labels, y_pred))\n",
        "print(\"Adjusted R2: \", adjusted_r2(r2_score(testing_labels, y_pred), n2, p2))\n",
        "\n",
        "# ValueError: Mean Squared Logarithmic Error cannot be used when targets contain negative values."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  4.6min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed: 10.5min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed: 19.5min\n",
            "[Parallel(n_jobs=-1)]: Done 1246 tasks      | elapsed: 31.3min\n",
            "[Parallel(n_jobs=-1)]: Done 1350 out of 1350 | elapsed: 33.9min finished\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Pipeline(memory=None,\n",
            "         steps=[('standardize',\n",
            "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
            "                ('model',\n",
            "                 LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
            "                               colsample_bytree=0.3, eval_metric='rmse',\n",
            "                               importance_type='split', learning_rate=0.1,\n",
            "                               max_depth=9, min_child_samples=20,\n",
            "                               min_child_weight=0.001, min_samples_leaf=2,\n",
            "                               min_split_gain=0.0, n_estimators=300, n_jobs=-1,\n",
            "                               num_leaves=31, objective=None, random_state=None,\n",
            "                               reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
            "                               subsample=0.7, subsample_for_bin=200000,\n",
            "                               subsample_freq=0))],\n",
            "         verbose=False)\n",
            "Training Errors\n",
            "Mean Squared Error:  0.05236697071909393\n",
            "RMSE:  0.2288383069311035\n",
            "Mean Absolute Error:  0.05840637214862351\n",
            "R-squared:  0.9476325904944419\n",
            "Adjusted R2:  0.9476264467117445\n",
            "Test Errors\n",
            "Mean Squared Error:  0.05968673906035098\n",
            "RMSE:  0.24430869624381155\n",
            "Mean Absolute Error:  0.06415639168013557\n",
            "R-squared:  0.9335835121117316\n",
            "Adjusted R2:  0.9335133150084223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C6_JSG1PZcx"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUIu2tC-9GWz"
      },
      "source": [
        "> With Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN881fId97-2"
      },
      "source": [
        "* Training Errors\n",
        "> Mean Squared Error:  0.006577338708038475\n",
        "> RMSE:  0.08110079351053524\n",
        "> Mean Absolute Error:  0.034764883555239344\n",
        "> R-squared:  0.9934226061799862\n",
        "> Adjusted R2:  0.9934218345153435\n",
        "* Test Errors\n",
        "> Mean Squared Error:  0.04780776840983414\n",
        "> RMSE:  0.21864987630875563\n",
        "> Mean Absolute Error:  0.04773988421430823\n",
        "> R-squared:  0.9468018504353819\n",
        "> Adjusted R2:  0.9467456240958149"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLpXwGafO5Rv",
        "outputId": "131b898b-005e-4023-b6d5-1cb9b210471f"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost\n",
        "from sklearn import preprocessing\n",
        "from xgboost import XGBRFRegressor\n",
        "\n",
        "# Initialise XGBRegressor\n",
        "rf = xgboost.XGBRegressor(n_estimators=300, subsample=0.9, colsample_bynode=0.2, n_fold = 5, eval_metric = 'rmse')\n",
        "\n",
        "# Set up 5-fold cross-validation\n",
        "from sklearn import model_selection\n",
        "cv = model_selection.KFold(5)\n",
        "\n",
        "# Pipeline standardization and Model\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n",
        "                           , ('model', rf) ])\n",
        "# Tuning the model\n",
        "my_min_samples_leaf = [2, 10, 25, 50, 100]\n",
        "my_max_depth = [7, 8, 9, 10, 11, 12]\n",
        "\n",
        "\n",
        "# Run the model using GridSearch, select the model with best search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "optimized_rf = GridSearchCV(estimator=pipeline\n",
        "                            , cv=cv\n",
        "                            , param_grid =dict(model__min_samples_leaf = my_min_samples_leaf, model__max_depth = my_max_depth)\n",
        "                            , scoring = 'neg_mean_squared_error'\n",
        "                            , verbose = 1\n",
        "                            , n_jobs = -1\n",
        "                           )\n",
        "\n",
        "# Fitting on Training Data\n",
        "optimized_rf.fit(training_set, training_labels)\n",
        "\n",
        "\n",
        "# Best model estimators\n",
        "print(optimized_rf.best_estimator_)\n",
        "\n",
        "def adjusted_r2(r2, n, p):\n",
        "  result = 1-((1-r2)*((n-1)/(n-p-1)))\n",
        "  return result\n",
        "\n",
        "\n",
        "# Evaluate metrics on holdout\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "y_train_pred = optimized_rf.predict(training_set)\n",
        "y_pred = optimized_rf.predict(testing_set)\n",
        "n1 = len(y_train_pred)\n",
        "p1 = training_set.shape[1]\n",
        "n2 = len(y_pred)\n",
        "p2 = testing_set.shape[1]\n",
        "print('Training Errors')\n",
        "print(\"Mean Squared Error: \", mean_squared_error(training_labels, y_train_pred))\n",
        "print(\"RMSE: \", mean_squared_error(training_labels, y_train_pred, squared = False))\n",
        "print(\"Mean Absolute Error: \", mean_absolute_error(training_labels, y_train_pred))\n",
        "print(\"R-squared: \", r2_score(training_labels, y_train_pred))\n",
        "print(\"Adjusted R2: \", adjusted_r2(r2_score(training_labels, y_train_pred), n1, p1))\n",
        "\n",
        "print('Test Errors')\n",
        "print(\"Mean Squared Error: \", mean_squared_error(testing_labels, y_pred))\n",
        "print(\"RMSE: \", mean_squared_error(testing_labels, y_pred, squared = False))\n",
        "print(\"Mean Absolute Error: \", mean_absolute_error(testing_labels, y_pred))\n",
        "print(\"R-squared: \", r2_score(testing_labels, y_pred))\n",
        "print(\"Adjusted R2: \", adjusted_r2(r2_score(testing_labels, y_pred), n2, p2))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 10.1min\n",
            "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 40.6min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[16:03:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Pipeline(memory=None,\n",
            "         steps=[('standardize',\n",
            "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
            "                ('model',\n",
            "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
            "                              colsample_bylevel=1, colsample_bynode=0.2,\n",
            "                              colsample_bytree=1, eval_metric='rmse', gamma=0,\n",
            "                              importance_type='gain', learning_rate=0.1,\n",
            "                              max_delta_step=0, max_depth=9, min_child_weight=1,\n",
            "                              min_samples_leaf=2, missing=None,\n",
            "                              n_estimators=300, n_fold=5, n_jobs=1,\n",
            "                              nthread=None, objective='reg:linear',\n",
            "                              random_state=0, reg_alpha=0, reg_lambda=1,\n",
            "                              scale_pos_weight=1, seed=None, silent=None,\n",
            "                              subsample=0.9, verbosity=1))],\n",
            "         verbose=False)\n",
            "Training Errors\n",
            "Mean Squared Error:  0.006577338708038475\n",
            "RMSE:  0.08110079351053524\n",
            "Mean Absolute Error:  0.034764883555239344\n",
            "R-squared:  0.9934226061799862\n",
            "Adjusted R2:  0.9934218345153435\n",
            "Test Errors\n",
            "Mean Squared Error:  0.04780776840983414\n",
            "RMSE:  0.21864987630875563\n",
            "Mean Absolute Error:  0.04773988421430823\n",
            "R-squared:  0.9468018504353819\n",
            "Adjusted R2:  0.9467456240958149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y1kJ9-OuXxZ"
      },
      "source": [
        "## Without normalisation of Price, but keeping normalised variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lC2ITI3-NYE"
      },
      "source": [
        "> Testing RMSE is $373260. This is an extremely large jump compared to previous models on non-normalised datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G66lDL1BgE15"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(combined.drop('price', axis=1),\n",
        "                                                    combined['price'], \n",
        "                                                    test_size=0.1,\n",
        "                                                    random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WRP4H3ogNoQ"
      },
      "source": [
        "# Normalise remaining lease for training and test\n",
        "X_train_lease_mean = X_train['remaining_lease'].mean()\n",
        "\n",
        "X_train_lease_std = X_train['remaining_lease'].std()\n",
        "X_train['remaining_lease'] = (X_train['remaining_lease'] - X_train_lease_mean)/X_train_lease_std\n",
        "\n",
        "X_test['remaining_lease'] = (X_test['remaining_lease'] - X_train_lease_mean)/X_train_lease_std\n",
        "\n",
        "\n",
        "# Normalise floor area for training and test\n",
        "X_train_area_mean = X_train['floor_area_sqm'].mean()\n",
        "\n",
        "X_train_area_std = X_train['floor_area_sqm'].std()\n",
        "X_train['floor_area_sqm'] = (X_train['floor_area_sqm'] - X_train_area_mean)/X_train_area_std\n",
        "\n",
        "X_test['floor_area_sqm'] = (X_test['floor_area_sqm'] - X_train_area_mean)/X_train_area_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEkIElUrqt2A"
      },
      "source": [
        "y_train /= 100000\n",
        "y_test /= 100000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td0AtXZ6gYXK",
        "outputId": "bc599ef9-b52c-4d8e-8c01-3355f274e2de"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import preprocessing\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "# Initialise LGBMRegressor\n",
        "# rf = LGBMRegressor(n_estimators=300, subsample=0.9, colsample_bytree=0.2, eval_metric = 'rmse')\n",
        "rf = LGBMRegressor(n_estimators=300, eval_metric = 'rmse')\n",
        "\n",
        "\n",
        "# Set up 5-fold cross-validation\n",
        "from sklearn import model_selection\n",
        "cv = model_selection.KFold(5)\n",
        "\n",
        "\n",
        "# Pipeline Standardization and Model\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n",
        "                           , ('model', rf) ])\n",
        "# Tuning the model\n",
        "my_min_samples_leaf = [2, 10, 25, 50, 100]\n",
        "my_max_depth = [7, 8, 9, 10, 11, 12]\n",
        "my_subsample = [0.7, 0.8, 0.9]\n",
        "my_colsample_bytree = [0.1, 0.2, 0.3]\n",
        "\n",
        "# Run the model using GridSearch, select the model with best search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "optimized_rf = GridSearchCV(estimator=pipeline\n",
        "                            , cv=cv\n",
        "                            , param_grid =dict(model__min_samples_leaf = my_min_samples_leaf, \n",
        "                                               model__max_depth = my_max_depth,\n",
        "                                               model__subsample = my_subsample,\n",
        "                                               model__colsample_bytree = my_colsample_bytree)\n",
        "                            , scoring = 'neg_mean_squared_error'\n",
        "                            , verbose = 1\n",
        "                            , n_jobs = -1\n",
        "                           )\n",
        "\n",
        "# Fitting on Training Data\n",
        "optimized_rf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Best model estimators\n",
        "print(optimized_rf.best_estimator_)\n",
        "\n",
        "\n",
        "# Evaluate metrics on holdout\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "y_train_pred = optimized_rf.predict(X_train)\n",
        "y_pred = optimized_rf.predict(X_test)\n",
        "print('Training Errors')\n",
        "print(\"Mean Squared Error: \", mean_squared_error(y_train, y_train_pred))\n",
        "#print(\"Mean Squared Log Error: \", mean_squared_log_error(training_labels, y_train_pred))\n",
        "print(\"Mean Absolute Error: \", mean_absolute_error(y_train, y_train_pred))\n",
        "print(\"R-squared: \", r2_score(y_train, y_train_pred))\n",
        "\n",
        "print('Test Errors')\n",
        "print(\"Mean Squared Error: \", mean_squared_error(y_test, y_pred))\n",
        "#print(\"Mean Squared Log Error: \", mean_squared_log_error(testing_labels, y_train_pred))\n",
        "print(\"Mean Absolute Error: \", mean_absolute_error(y_test, y_pred))\n",
        "print(\"R-squared: \", r2_score(y_test, y_pred))\n",
        "\n",
        "# ValueError: Mean Squared Logarithmic Error cannot be used when targets contain negative values."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  4.4min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed: 10.0min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed: 18.3min\n",
            "[Parallel(n_jobs=-1)]: Done 1246 tasks      | elapsed: 29.0min\n",
            "[Parallel(n_jobs=-1)]: Done 1350 out of 1350 | elapsed: 31.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Pipeline(memory=None,\n",
            "         steps=[('standardize',\n",
            "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
            "                ('model',\n",
            "                 LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
            "                               colsample_bytree=0.3, eval_metric='rmse',\n",
            "                               importance_type='split', learning_rate=0.1,\n",
            "                               max_depth=7, min_child_samples=20,\n",
            "                               min_child_weight=0.001, min_samples_leaf=2,\n",
            "                               min_split_gain=0.0, n_estimators=300, n_jobs=-1,\n",
            "                               num_leaves=31, objective=None, random_state=None,\n",
            "                               reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
            "                               subsample=0.7, subsample_for_bin=200000,\n",
            "                               subsample_freq=0))],\n",
            "         verbose=False)\n",
            "Training Errors\n",
            "Mean Squared Error:  16.57098283180115\n",
            "Mean Absolute Error:  1.0027637343707159\n",
            "R-squared:  0.9377143821831225\n",
            "Test Errors\n",
            "Mean Squared Error:  13.93236433052307\n",
            "Mean Absolute Error:  1.0680404022619068\n",
            "R-squared:  0.9417281579240565\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}