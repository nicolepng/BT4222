{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import linalg, optimize\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "from nltk.corpus import wordnet as wn\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk import word_tokenize\n",
    "from scipy import optimize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import coo_matrix, hstack, vstack\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "# Graphing: \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = '/Users/vickiyew/Desktop/bt4222_data/combined_psf.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform into DataFrame and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district</th>\n",
       "      <th>street</th>\n",
       "      <th>propertyType</th>\n",
       "      <th>remaining_lease</th>\n",
       "      <th>school</th>\n",
       "      <th>hawkercentre</th>\n",
       "      <th>supermarkets</th>\n",
       "      <th>Bus Stops Nearby</th>\n",
       "      <th>crime_number</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>floor_range</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>floor_area_sqft</th>\n",
       "      <th>price_psf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>ZEHNDER ROAD</td>\n",
       "      <td>Semi-detached</td>\n",
       "      <td>999.00</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.282130</td>\n",
       "      <td>103.786879</td>\n",
       "      <td>~-</td>\n",
       "      <td>0.380502</td>\n",
       "      <td>5643.51277</td>\n",
       "      <td>974.570312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>ZEHNDER ROAD</td>\n",
       "      <td>Semi-detached</td>\n",
       "      <td>999.00</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.282130</td>\n",
       "      <td>103.786879</td>\n",
       "      <td>~-</td>\n",
       "      <td>0.123495</td>\n",
       "      <td>3315.28120</td>\n",
       "      <td>1508.167693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>ZEHNDER ROAD</td>\n",
       "      <td>Semi-detached</td>\n",
       "      <td>999.00</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.282130</td>\n",
       "      <td>103.786879</td>\n",
       "      <td>~-</td>\n",
       "      <td>0.123495</td>\n",
       "      <td>3379.86460</td>\n",
       "      <td>1405.381742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>NEO PEE TECK LANE</td>\n",
       "      <td>Terrace</td>\n",
       "      <td>999.00</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.292047</td>\n",
       "      <td>103.768591</td>\n",
       "      <td>~-</td>\n",
       "      <td>0.380502</td>\n",
       "      <td>1714.68927</td>\n",
       "      <td>1533.805597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>COVE DRIVE</td>\n",
       "      <td>Condominium</td>\n",
       "      <td>85.25</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.244209</td>\n",
       "      <td>103.827487</td>\n",
       "      <td>~01-05</td>\n",
       "      <td>0.173732</td>\n",
       "      <td>2088.19660</td>\n",
       "      <td>1522.845119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   district             street   propertyType  remaining_lease  school  \\\n",
       "0         5       ZEHNDER ROAD  Semi-detached           999.00      27   \n",
       "1         5       ZEHNDER ROAD  Semi-detached           999.00      27   \n",
       "2         5       ZEHNDER ROAD  Semi-detached           999.00      27   \n",
       "3         5  NEO PEE TECK LANE        Terrace           999.00      27   \n",
       "4         4         COVE DRIVE    Condominium            85.25      11   \n",
       "\n",
       "   hawkercentre  supermarkets  Bus Stops Nearby  crime_number  latitude  \\\n",
       "0             6             8                 0          -1.0  1.282130   \n",
       "1             6             8                 0          -1.0  1.282130   \n",
       "2             6             8                 0          -1.0  1.282130   \n",
       "3             6             8                 2          -1.0  1.292047   \n",
       "4             3             2                 0          -1.0  1.244209   \n",
       "\n",
       "    longitude floor_range  sentiment  floor_area_sqft    price_psf  \n",
       "0  103.786879          ~-   0.380502       5643.51277   974.570312  \n",
       "1  103.786879          ~-   0.123495       3315.28120  1508.167693  \n",
       "2  103.786879          ~-   0.123495       3379.86460  1405.381742  \n",
       "3  103.768591          ~-   0.380502       1714.68927  1533.805597  \n",
       "4  103.827487      ~01-05   0.173732       2088.19660  1522.845119  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(combined)\n",
    "df.drop(df.columns[0], axis = 1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['floor_area_sqft'] = 10.7639 * df['floor_area_sqm']\n",
    "# df['price_psf'] = df['price'] / df['floor_area_sqft']\n",
    "# cols = ['floor_area_sqm', 'price']\n",
    "# df.drop(cols, axis = 1, inplace = True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('combined_psf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['street'] = df['street'].astype('category')\n",
    "street_dict = dict(zip(df['street'].cat.codes, df['street']))\n",
    "df['street'] = df['street'].cat.codes\n",
    "\n",
    "df['propertyType'] = df['propertyType'].astype('category')\n",
    "property_dict = dict(zip(df['propertyType'].cat.codes, df['propertyType']))\n",
    "df['propertyType'] = df['propertyType'].cat.codes\n",
    "\n",
    "df['floor_range'] = df['floor_range'].astype('category')\n",
    "floor_dict = dict(zip(df['floor_range'].cat.codes, df['floor_range']))\n",
    "df['floor_range'] = df['floor_range'].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('price_psf', axis=1),\n",
    "                                                    df['price_psf'], \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Cross - Latitude & Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, longitude_boundaries = np.histogram(X_train['longitude'])\n",
    "_, latitude_boundaries = np.histogram(X_train['latitude'])\n",
    "\n",
    "def assign_bucket(x, ranges):\n",
    "    if x <= ranges[0]:\n",
    "        return 0\n",
    "    \n",
    "    for i in range(1, len(ranges) - 1):\n",
    "        if x <= ranges[i]:\n",
    "            return i - 1\n",
    "    return len(ranges) - 2\n",
    "\n",
    "X_train['lgt_discrete'] = X_train['longitude'].apply(lambda x: assign_bucket(x, longitude_boundaries))\n",
    "X_train['lat_discrete'] = X_train['latitude'].apply(lambda x: assign_bucket(x, latitude_boundaries))\n",
    "X_test['lgt_discrete'] = X_test['longitude'].apply(lambda x: assign_bucket(x, longitude_boundaries))\n",
    "X_test['lat_discrete'] = X_test['latitude'].apply(lambda x: assign_bucket(x, latitude_boundaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(categories='auto')\n",
    "features_lat_lgn_train = enc.fit_transform(X_train[['lgt_discrete', 'lat_discrete']]) # fit and transform TRAIN\n",
    "features_lat_lgn_test = enc.transform(X_test[['lgt_discrete', 'lat_discrete']]) # transform only TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = ['longitude', 'latitude', 'lgt_discrete', 'lat_discrete']\n",
    "OHE_train = np.hstack((X_train.drop(features_to_drop, axis=1).values, features_lat_lgn_train.toarray()))\n",
    "OHE_test = np.hstack((X_test.drop(features_to_drop, axis=1).values, features_lat_lgn_test.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Modelling & RandomisedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:26:54] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:26:54] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, n_fold } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Training Errors\n",
      "Mean Squared Error:  3435.9080167464085\n",
      "RMSE:  58.61661894673224\n",
      "Mean Absolute Error:  34.35651682894949\n",
      "R-squared:  0.9865797894749031\n",
      "Adjusted R2:  0.9865777814946497\n",
      "Test Errors\n",
      "Mean Squared Error:  7152.833953893002\n",
      "RMSE:  84.57442848694281\n",
      "Mean Absolute Error:  43.29525721575606\n",
      "R-squared:  0.9724662874083924\n",
      "Adjusted R2:  0.9724498007723583\n",
      "Training sMAPE\n",
      "4.931522803813893\n",
      "Testing sMAPE\n",
      "5.6877317228539415\n",
      "Training MAPE\n",
      "4.921068971751144\n",
      "Testing MAPE\n",
      "5.67108578966238\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# Initialise LGBMRegressor\n",
    "# rf = LGBMRegressor(n_estimators=300, subsample=0.9, colsample_bytree=0.2, eval_metric = 'rmse')\n",
    "optimized_rf = XGBRegressor(base_score=0.5, booster='gbtree',\n",
    "                              colsample_bylevel=1, colsample_bynode=0.2,\n",
    "                              colsample_bytree=1, eval_metric='rmse', gamma=0,\n",
    "                              importance_type='gain', learning_rate=0.1,\n",
    "                              max_delta_step=0, max_depth=10,\n",
    "                              min_child_weight=1, min_samples_leaf=2,\n",
    "                              missing=None, n_estimators=300, n_fold=5,\n",
    "                              n_jobs=1, nthread=None, objective='reg:linear',\n",
    "                              random_state=0, reg_alpha=0, reg_lambda=1,\n",
    "                              scale_pos_weight=1, seed=None, silent=None,\n",
    "                              subsample=0.9, verbosity=1)\n",
    "\n",
    "\n",
    "# # # Set up 5-fold cross-validation\n",
    "# from sklearn import model_selection\n",
    "# cv = model_selection.KFold(5)\n",
    "\n",
    "\n",
    "# # Pipeline Standardization and Model\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# pipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n",
    "#                            , ('model', rf) ])\n",
    "# # Tuning the model\n",
    "# my_min_samples_leaf = [2, 10, 25, 50, 100]\n",
    "# my_max_depth = [7, 8, 9, 10, 11, 12]\n",
    "# my_subsample = [0.7, 0.8, 0.9]\n",
    "# my_colsample_bytree = [0.1, 0.2, 0.3]\n",
    "\n",
    "# # Run the model using GridSearch, select the model with best search\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# optimized_rf = GridSearchCV(estimator=pipeline\n",
    "#                             , cv=cv\n",
    "#                             , param_grid =dict(model__min_samples_leaf = my_min_samples_leaf, \n",
    "#                                                model__max_depth = my_max_depth,\n",
    "#                                                model__subsample = my_subsample,\n",
    "#                                                model__colsample_bytree = my_colsample_bytree)\n",
    "#                             , scoring = 'neg_mean_squared_error'\n",
    "#                             , verbose = 1\n",
    "#                             , n_jobs = -1\n",
    "#                            )\n",
    "\n",
    "# Fitting on Training Data\n",
    "optimized_rf.fit(OHE_train, y_train)\n",
    "\n",
    "\n",
    "# Best model estimators\n",
    "# print(optimized_rf.best_estimator_)\n",
    "\n",
    "def adjusted_r2(r2, n, p):\n",
    "  result = 1-((1-r2)*((n-1)/(n-p-1)))\n",
    "  return result\n",
    "\n",
    "\n",
    "# Evaluate metrics on holdout\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "y_train_pred = optimized_rf.predict(OHE_train)\n",
    "y_pred = optimized_rf.predict(OHE_test)\n",
    "n1 = len(y_train_pred)\n",
    "p1 = X_train.shape[1]\n",
    "n2 = len(y_pred)\n",
    "p2 = X_test.shape[1]\n",
    "print('Training Errors')\n",
    "print(\"Mean Squared Error: \", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"RMSE: \", mean_squared_error(y_train, y_train_pred, squared = False))\n",
    "print(\"Mean Absolute Error: \", mean_absolute_error(y_train, y_train_pred))\n",
    "print(\"R-squared: \", r2_score(y_train, y_train_pred))\n",
    "print(\"Adjusted R2: \", adjusted_r2(r2_score(y_train, y_train_pred), n1, p1))\n",
    "\n",
    "print('Test Errors')\n",
    "print(\"Mean Squared Error: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: \", mean_squared_error(y_test, y_pred, squared = False))\n",
    "print(\"Mean Absolute Error: \", mean_absolute_error(y_test, y_pred))\n",
    "print(\"R-squared: \", r2_score(y_test, y_pred))\n",
    "print(\"Adjusted R2: \", adjusted_r2(r2_score(y_test, y_pred), n2, p2))\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 1/len(y_true) * np.sum(2 * np.abs(y_pred.squeeze()-y_true.squeeze()) / (np.abs(y_true.squeeze()) + np.abs(y_pred.squeeze()))*100)\n",
    "\n",
    "\n",
    "print('Training sMAPE')\n",
    "print(smape(y_train_pred, y_train))\n",
    "print('Testing sMAPE')\n",
    "print(smape(y_pred, y_test))\n",
    "\n",
    "def percentage_error(actual, predicted):\n",
    "    res = np.empty(actual.shape)\n",
    "    for j in range(actual.shape[0]):\n",
    "        if actual[j] != 0:\n",
    "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
    "        else:\n",
    "            res[j] = predicted[j] / np.mean(actual)\n",
    "    return res\n",
    "\n",
    "def mape(y_true, y_pred): \n",
    "    return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100\n",
    "\n",
    "print('Training MAPE')\n",
    "print(mape(y_train_pred, y_train))\n",
    "print('Testing MAPE')\n",
    "print(mape(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'saved_model.sav'\n",
    "# pickle.dump(optimized_rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(OHE_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['district', 'street', 'propertyType', 'remaining_lease', 'school', 'hawkercentre', 'supermarkets', 'Bus Stops Nearby', 'crime_number', 'floor_range', 'sentiment', 'floor_area_sqft']\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "test = X_train.drop(features_to_drop, axis=1)\n",
    "new_col = test.columns.tolist()\n",
    "print(new_col)\n",
    "print(len(new_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretability - SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = new_col\n",
    "cols += [str(i) for i in range(0, 20)]\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:27:38] WARNING: /Users/travis/build/dmlc/xgboost/src/objective/regression_obj.cu:170: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[18:27:38] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { min_samples_leaf, n_fold } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=0.2, colsample_bytree=1, eval_metric='rmse',\n",
       "             gamma=0, gpu_id=-1, importance_type='gain',\n",
       "             interaction_constraints='', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=10, min_child_weight=1, min_samples_leaf=2, missing=None,\n",
       "             monotone_constraints='()', n_estimators=300, n_fold=5, n_jobs=1,\n",
       "             nthread=1, num_parallel_tree=1, objective='reg:linear',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=0, silent=None, subsample=0.9, ...)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.DataFrame(OHE_train, columns = cols)\n",
    "df_test = pd.DataFrame(OHE_test, columns = cols)\n",
    "optimized_rf.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(optimized_rf)\n",
    "shap_values = explainer(df_test)\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Initialise LGBMRegressor\n",
    "rf = LGBMRegressor(n_estimators=300, subsample=0.9, colsample_bytree=0.2, eval_metric = 'rmse')\n",
    "\n",
    "\n",
    "# # Set up 5-fold cross-validation\n",
    "from sklearn import model_selection\n",
    "cv = model_selection.KFold(5)\n",
    "\n",
    "\n",
    "# Pipeline Standardization and Model\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps=[('standardize', preprocessing.StandardScaler())\n",
    "                           , ('model', rf) ])\n",
    "# Tuning the model\n",
    "my_min_samples_leaf = [2, 10, 25, 50, 100]\n",
    "my_max_depth = [7, 8, 9, 10, 11, 12]\n",
    "my_subsample = [0.7, 0.8, 0.9]\n",
    "my_colsample_bytree = [0.1, 0.2, 0.3]\n",
    "\n",
    "# Run the model using GridSearch, select the model with best search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "optimized_rf = RandomizedSearchCV(estimator=pipeline\n",
    "                            , cv=cv\n",
    "                            , param_distributions =dict(model__min_samples_leaf = my_min_samples_leaf, \n",
    "                                               model__max_depth = my_max_depth,\n",
    "                                               model__subsample = my_subsample,\n",
    "                                               model__colsample_bytree = my_colsample_bytree)\n",
    "                            , scoring = 'neg_mean_squared_error'\n",
    "                            , verbose = 1\n",
    "                            , n_jobs = -1\n",
    "                           )\n",
    "\n",
    "# Fitting on Training Data\n",
    "optimized_rf.fit(OHE_train, y_train)\n",
    "\n",
    "\n",
    "# Best model estimators\n",
    "print(optimized_rf.best_estimator_)\n",
    "\n",
    "def adjusted_r2(r2, n, p):\n",
    "  result = 1-((1-r2)*((n-1)/(n-p-1)))\n",
    "  return result\n",
    "\n",
    "\n",
    "# Evaluate metrics on holdout\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "y_train_pred = optimized_rf.predict(OHE_train)\n",
    "y_pred = optimized_rf.predict(OHE_test)\n",
    "n1 = len(y_train_pred)\n",
    "p1 = X_train.shape[1]\n",
    "n2 = len(y_pred)\n",
    "p2 = X_test.shape[1]\n",
    "print('Training Errors')\n",
    "print(\"Mean Squared Error: \", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"RMSE: \", mean_squared_error(y_train, y_train_pred, squared = False))\n",
    "print(\"Mean Absolute Error: \", mean_absolute_error(y_train, y_train_pred))\n",
    "print(\"R-squared: \", r2_score(y_train, y_train_pred))\n",
    "print(\"Adjusted R2: \", adjusted_r2(r2_score(y_train, y_train_pred), n1, p1))\n",
    "\n",
    "print('Test Errors')\n",
    "print(\"Mean Squared Error: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: \", mean_squared_error(y_test, y_pred, squared = False))\n",
    "print(\"Mean Absolute Error: \", mean_absolute_error(y_test, y_pred))\n",
    "print(\"R-squared: \", r2_score(y_test, y_pred))\n",
    "print(\"Adjusted R2: \", adjusted_r2(r2_score(y_test, y_pred), n2, p2))\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 1/len(y_true) * np.sum(2 * np.abs(y_pred.squeeze()-y_true.squeeze()) / (np.abs(y_true.squeeze()) + np.abs(y_pred.squeeze()))*100)\n",
    "\n",
    "\n",
    "print('Training sMAPE')\n",
    "print(smape(y_train_pred, y_train))\n",
    "print('Testing sMAPE')\n",
    "print(smape(y_pred, y_test))\n",
    "\n",
    "def percentage_error(actual, predicted):\n",
    "    res = np.empty(actual.shape)\n",
    "    for j in range(actual.shape[0]):\n",
    "        if actual[j] != 0:\n",
    "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
    "        else:\n",
    "            res[j] = predicted[j] / np.mean(actual)\n",
    "    return res\n",
    "\n",
    "def mape(y_true, y_pred): \n",
    "    return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100\n",
    "\n",
    "print('Training MAPE')\n",
    "print(mape(y_train_pred, y_train))\n",
    "print('Testing MAPE')\n",
    "print(mape(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
